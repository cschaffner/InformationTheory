\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{5}
\newcommand\deadline{Wed, 5 December 2018, 12:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}

\begin{document}

\homeworkproblems

{\sffamily\noindent
Unless otherwise stated, you should provide exact answers rather than rounded numbers (e.g., $\log 3$ instead of 1.585) for non-programming exercises.
}

\begin{exercise}[Bottleneck (4pt)]
Suppose a Markov chain starts in one of $n$ states, necks down to $k < n$ states, and then fans back to $m > k$ states. Thus $X_1 \to X_2 \to X_3$, with $\mathcal{X}_1 = \{1, 2, ..., n\}$, $\mathcal{X}_2 = \{1, 2, ..., k\}$, and $\mathcal{X}_3 = \{1, 2, ..., m\}$.
\begin{subex}[(3pt)]
Show that the dependence of $X_1$ and $X_3$ is limited by the bottleneck by proving that $I(X_1;X_3) \leq \log k$.
\end{subex}
\begin{subex}[(1pt)]
Evaluate $I(X_1;X_3)$ for $k = 1$, and conclude that no dependence can survive such a bottleneck.
\end{subex}
\end{exercise}






\begin{exercise}[A dog looking for a bone (6pt)]
A dog walks on the integers, possibly reversing direction at each step with probability $p = 0.1$. Let $X_0 = 0$. The first step is equally likely to be positive or negative. A typical walk might look like this:
\[
(X_0, X_1, ...) = (0, -1, -2, -3, -4, -3, -2, -1, 0, 1, ...).
\]
	\begin{subex}[(2pt)]
	What is the expected number of steps the dog takes in one direction, before reversing?
	\end{subex}
	\begin{subex}[(1pt)]
	Is this a Markov process? If so, is it time-invariant?
	\end{subex}
	\begin{subex}[(3pt)]
	Find the entropy rate of this browsing dog.
	\end{subex}
\end{exercise}

\begin{exercise}[Run-length coding (3pt)]
Let $X_1, X_2, ..., X_n$ be (possibly dependent) binary random variables.
Suppose one calculates the run lengths $R = (R_1, R_2, ...)$ of this sequence (in order as they occur).
For example, the sequence $X = 0001100100$ yields run lengths $R = (3, 2, 2, 1, 2)$. Compare
$H(X_1, X_2, . . . , X_n)$, $H(R)$ and $H(X_n, R)$. Show all equalities and inequalities, and bound all the
differences.
\end{exercise}

\begin{exercise}[Entropy rates of Markov chains (10pt)]
	\begin{subex}[(2pt)]
	Find the entropy rate of the two-state Markov chain with transition matrix
	\[
	P = \left[
	\begin{array}{c c}
	1-p_{ab} & p_{ab}\\
	p_{ba} & 1 - p_{ba}
	\end{array}
	\right].
	\]
	\end{subex}
	\begin{subex}[(1pt)]
	Find values of $p_{ab}$ and $p_{ba}$ that maximize the entropy rate of part (a).
	\end{subex}
	\begin{subex}[(1pt)]
	Find the entropy rate of the two-state Markov chain with transition matrix
	\[
	P = \left[
	\begin{array}{c c}
	1-p & p\\
	1 & 0
	\end{array}
	\right].
	\]
	\end{subex}
	\begin{subex}[(2pt)]
	Find the maximum value of the entropy rate of for part (c). \\\textbf{Hint:} we expect that the maximizing value of $p$ should be less than 1/2, since the 0 state permits more information to be generated than the 1 state. This allows you to discard one possible solution.
	\end{subex}
	\begin{subex}[(4pt)]
	Let $N(t)$ be the number of allowable state sequences of length $t$ for the Markov chain of part (c). Find $N(t)$ and calculate
	\[
	H_0 := \lim_{t \to \infty} \frac{1}{t} \log N(t).
	\]
	Why is $H_0$ an upper bound on the entropy rate of the Markov chain? Compare $H_0$ with the maximum entropy found in (d).
	\\\textbf{Hint:} Find $N(0)$ and $N(1)$, and find a linear \href{https://en.wikipedia.org/wiki/Recurrence_relation}{recurrence} that expresses $N(t)$ in terms of $N(t-1)$ and $N(t-2)$. What well-known sequence does this remind you of? You can use any known properties of this sequence.
	\end{subex}
\end{exercise}






\end{document}