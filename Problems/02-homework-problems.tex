\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{Wednesday, 21 November 2018, 12:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}

\begin{document}

\homeworkproblems

\begin{exercise}[Fun with dice (6 pt)]
Consider the following random experiment with two fair (regular six-sided) dice. First, the first die is thrown -- let the outcome be $A$. Then, the second die is thrown until the outcome has the same parity (even, odd) as $A$. Let this final outcome of the second die be $B$. The random variables $X$, $Y$ and $Z$ are defined as follows:
\[
X = (A + B) \mod 2, \ \ \ \ \ Y = (A \cdot B) \mod 2, \ \ \ \ \ Z = |A - B|.
\]
	\begin{subex}[(1pt)]
	Find the joint distribution $P_{AB}$.
	\end{subex}
	\begin{subex}[(1pt)]
	Determine $H(X), H(Y)$ and $H(Z)$.
	\end{subex}
	\begin{subex}[(1pt)]
	Compute $H(Z|A=1)$.
	\end{subex}
	\begin{subex}[(1pt)]
	Compute $H(AB)$, i.e.\ the joint entropy of $A$ and $B$.
	\end{subex}
	\begin{subex}[(2pt)]
	A random variable $M$ describes whether the sum $A + B$ is strictly larger than seven, between five and seven (both included) or scrictly smaller than five. How much entropy is present in $M$?
	\end{subex}
\end{exercise}

\begin{exercise}[Mutual information (2pt)]
Let the binary random variables $X$ and $Y$ be defined by the joint probability distribution $P_{XY}(00) = 0.3$, $P_{XY}(01) = 0.1$, $P_{XY}(10) = 0.2$, $P_{XY}(11) = 0.4$.

%in the following table:
%\begin{center}
%\begin{tabular}{c | c | c}
%& $X = 0$ & $X = 1$\\\hline
%$Y = 0$ & 0.3 & 0.2\\
%$Y = 1$ & 0.1 & 0.4
%\end{tabular}
%\end{center}
Draw the entropy diagram for $X$ and $Y$, and compute $H(X)$, $H(Y)$, $H(XY)$, $H(X|Y)$, $H(Y|X)$, and $I(X;Y)$. Reading off a value from the diagram is a valid `computation', as long as you clearly state how you did it.
\begin{center}
\entropydiagramXY[1]{?}{?}{?}{?}{?}{?}
\end{center}
\end{exercise}


\begin{exercise}[Entropy of functions of a random variable (3pt)]
Let $X$ be a random variable, and let $f$ be a function of $X$.
	\begin{subex}[(1pt)]
	Show that $H(f(X) | X) = 0$.
	\end{subex}
	\begin{subex}[(2pt)]
	Show that $H(f(X)) \leq H(X)$. \emph{Hint:} use the chain rule.
	\end{subex}
\end{exercise}


\begin{exercise}[Relative entropy (9pt)]
This exercise is about relative entropy, as defined in Section 1.7 of the \href{https://github.com/cschaffner/InformationTheory/raw/master/Script/InfTheory3.pdf}{lecture notes}. 
	\begin{subex}[(5pt)]
	Prove that for any two distributions $P$ and $Q$ over $\mathcal{X}$, $D(P||Q) \geq 0$, and that equality holds if and only if $P = Q$.
	\end{subex}
	\begin{subex}[(3pt)]
	Show that the mutual information can be expressed in terms of the relative entropy, i.e.\ that $I(X;Y) = D(P_{XY}||P_XP_Y)$.
	\end{subex}
	\begin{subex}[(1pt)]
	Use (a) and (b) to show that $H(X|Y) \leq H(X)$.
	\end{subex}
\end{exercise}



\begin{exercise}[Programming... (5pt)]
TODO: compute the entropy of a big distribution
\end{exercise}





\end{document}