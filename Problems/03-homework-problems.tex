\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{3}
\newcommand\deadline{Wednesday, 21 November 2018, 12:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}

\begin{document}

\homeworkproblems


\begin{exercise}[Huffman Coding]
	\begin{subex}[(4pt)]
    Consider the binary source $P_X$ with $P_X(0) = \frac{1}{8}$ and $P_X(1) = \frac{7}{8}$.
    
    Design a Huffman code for $P_X$. Describe, in order, which source symbols you combine, and list the final codebook. What is the average codeword length?
	\end{subex}
\begin{subex}[(4pt)]
	Design a Huffman code for blocks of $N=2$ bits drawn from the source $P_X$. Describe, in order, which source symbols you combine, and list the final codebook. What is the average codeword length?
\end{subex}
\begin{subex}[(4pt)]
	Design a Huffman code for blocks of $N=3$ bits drawn from the source $P_X$. Describe, in order, which source symbols you combine, and list the final codebook. What is the average codeword length?
\end{subex}
\begin{subex}[(4pt)]
	For the three codes you designed ($N=1,2,3$), divide the average codeword length by $N$, and compare these values to the optimal length, i.e., $H(X)$. What do you observe?
\end{subex}
	\begin{subex}[(1pt)]
	If you were asked to design a Huffman code for a block of $N = 100$ bits, what problem would you run into?
	\end{subex}
	\begin{subex}[(2pt)]
	Consider the random variable $Z$ with
	\begin{center}
	\begin{tabular}{c | c c c c c c}
	$z$ & 1 & 2 & 3 & 4 & 5 & 6\\
	\hline
	$P_Z(z)$ & 1/10 & 3/10 & 2/10 & 2/10 & 1/10 & 1/10\\
	\end{tabular}
	\end{center}
	Design a \emph{ternary} Huffman code for $Z$ (i.e. using an alphabet with three symbols).
	\end{subex}
\end{exercise}



\begin{exercise}[Inefficiency when using the wrong code]
\begin{subex}[(2pt)]
Given are two distributions $P_X$ and $Q_X$ for the same set $\mathcal{X} = \{\mathtt{a,b,c,d}\}$:
\[
\begin{array}{c | c c c c}
x & \mathtt{a}&\mathtt{b}&\mathtt{c}&\mathtt{d}\\\hline
P_X(x) & 1/4&1/4&1/4&1/4\\
Q_X(x) & 1/2&1/4&1/8&1/8\\
\end{array}
\]
Design an optimal prefix-free code for $Q_X(x)$.
\end{subex}
\begin{subex}[(4pt)]
What is the expected codeword length of the code you just designed? (Three decimals precision)
\end{subex}
\begin{subex}[(4pt)]
What is the expected codeword length if you use this code to encode the source $P_X$? (Three decimals precision)
\end{subex}
\begin{subex}[(4pt)]
Show that in general, when using optimal code for any source source $Q_X$ (which has lengths $\ell(x) = \lceil -\log Q_X(x) \rceil$ for all $x \in \mathcal{X}$) to encode another source $P_X$, this incurs a penalty of $D(P_X||Q_X)$ in the average description length.

More formally, prove that
\[
H(P_X) + D(P_X||Q_X) \leq \mathbb{E}_{P_X}[\ell(X)] \leq H(P_X) + D(P_X||Q_X) + 1
\]
for all $P_X$ and $Q_X$.
\end{subex}
\end{exercise}




\begin{exercise}[Shannon code (6pt)]
Consider the following method for generating a code for a random variable $X$ which takes on $m$ values $\{1,2,...,m\}$. Assume that the probabilities are ordered such that $P_X(1) \geq P_X(2) \geq ... \geq P_X(m)$. Define
\[
F_i := \sum_{k=1}^{i-1} P_X(k),
\]
the sum of the probabilities of all symbols less than $i$. Then the Shannon code is defined by assigning the (binary representation of the) number $F_i \in [0,1]$ as the codeword for $i$, where $F_i$ is rounded off to $\lceil \log\frac{1}{P_X(i)}\rceil$ bits.
	\begin{subex}[(1pt)]
	Construct the code for the probability distribution $P_X(1) = \frac{1}{2}$, $P_X(2) = \frac{1}{4}$, $P_X(3) = P_X(4) = \frac{1}{8}$
	\end{subex}
	\begin{subex}[(1pt)]
	Construct the code for the probability distribution $P_Y(1) = P_Y(2) = P_Y(3) = \frac{1}{3}$.
	\end{subex}
	\begin{subex}[(2pt)]
	Show that the Shannon code is prefix-free.
	\end{subex}
	\begin{subex}[(2pt)]
	Show that the average length $\ell_S$ of the Shannon code satisfies
	\[
	H(X) \leq \ell_S(P_X) < H(X) + 1.
	\]
	\end{subex}
\end{exercise}

\begin{exercise}[Sampling from any distribution using random bits]
In this exercise, we come up with a strategy to sample from an arbitrary distribution $P_X$ using fair random bits (for example, the outcome of a sequence of fair coin tosses).
	\begin{subex}
	Let $Z_1$ be a random variable with $\mathcal{Z}_1 = \{a,b,c\}$ and $P_{Z_1}(a) = 1/2$, $P_{Z_1}(b) = P_{Z_1}(c) = 1/4$. Come up with a strategy to sample from $X$ using a number of fair coin tosses. How many coin tosses do you expect to do? How does this compare to the entropy of $Z_1$?
	\end{subex}
	\begin{subex}
	Consider the standard binary representation of some $p_i \in [0,1)$. Let the \emph{atoms} of this representation be the set $At_i := \{2^{-k} \mid \mbox{ the } k^{th} \mbox{ bit of the binary representation of } p_i \mbox{ is 1.}\}$. Find the atoms for the binary expansion of $p_1 = \frac{1}{3}$ and $p_2 = \frac{2}{3}$.
	\end{subex}
	\begin{subex}
	Show that for any probability distribution with probabilities $(p_1, ..., p_n)$, it is possible to construct a binary tree (the \emph{sampling tree} for this distribution) such that if $2^{-k} \in At_i$ for some $i$, then the tree contains a leaf with label $i$ at depth $k$. \textbf{Hint:} use Kraft's inequality.
	\end{subex}
	\begin{subex}
	Let $Z_2$ be a random variable with $\mathcal{Z}_2 = \{a,b\}$ and $P_{Z_2}(a) = 1/3$, $P_{Z_2}(b) = 2/3$. Construct the sampling tree for $P_{Z_2}$. Find a fair coin and use it to sample from this distribution, following the strategy described by the sampling tree.
	\end{subex}
Let $ET(X)$ denote the expected number of coin tosses when sampling from $X$ using the sampling tree described above. In the rest of this exercise, you will show that this method of sampling from an arbitrary distribution $P_X$ using fair random bits is quite efficient in terms of $ET(X)$.
	\begin{subex}
	Given a sampling tree for an arbitrary distribution $P_X$, define a random variable $Y$ with $\mathcal{Y}$ the set of all leafs of the tree, and $P_Y(y) = 2^{-d(y)}$, where $d(y)$ is the depth of the leaf $y$ in the tree. Prove that $H(Y) = ET(X)$.
	\end{subex}
	\begin{subex}
	Use the result from (e) to prove that $H(X) \leq ET(X)$.
	\end{subex}
	\begin{subex**}
	Prove that $H(Y|X) < 2$ (\textbf{Hint:} see Cover and Thomas, Section 5.12)
	\end{subex**}
	\begin{subex}
	Use the result from $\bigstar$ to prove that $ET(X) < H(X) + 2$.
	\end{subex}
\end{exercise}


\begin{exercise}[Programming project (10pt)]
Invent a compressor and uncompressor for a source file of $N=10000$
bits each having probability $p=0.01$ of being a $1$. Implement them
and estimate how well your method works.

Evaluate the performance of your algorithm on \href{https://github.com/cschaffner/InformationTheory/raw/master/Problems/random01.txt}{this particular
  file}. Use \href{https://repl.it/@ChrisSchaffner/RandomFile}{this program} to generate more testdata.
\end{exercise}



\end{document}