\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{...}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\begin{document}

\practiceproblems

{\sffamily\noindent
%This week's exercises deal with sets, counting and uniform probabilities.
This week's exercises deal with entropy. You do not have to hand in these exercises, they are for practicing only. Problems marked with a $\bigstar$ are generally a bit harder. If you have questions about any of the exercises, please post them in the \href{https://canvas.uva.nl/courses/2205/discussion_topics/22949}{discussion forum on Canvas}, and try to help each other. We will also keep an eye on the forum.
}

\begin{exercise}[Properties of entropy]
Let $X$ and $Y$ be random variables.
	\begin{subex}
	Prove that $H(X) = 0$ if and only if $X$ is \emph{constant}, i.e. there is an $x_0 \in \mathcal{X}$ such that $P_X(x_0) = 1$, and $P_X(x') = 0$ for all $x' \neq x_0$.
	\end{subex}
	
	\begin{subex}
	Prove that $H(XY) = H(X) + H(Y)$ if and only if $X$ and $Y$ are independent.
	\end{subex}

	\begin{subex}
	Prove that $H(X) = \log |\mathcal{X}|$ if $X$ is uniformly distributed.
	\end{subex}

	\begin{subex**}
	Prove that $X$ is uniformly distributed if $H(X) = \log |\mathcal{X}|$.
	\end{subex**}
	
\end{exercise}

\begin{exercise}[Entropy of a deck of cards]
	\begin{subex}
	Compute the entropy of a perfectly shuffled deck of 52 cards (i.e.\ the set of cards is uniformly distributed over all possible orders).
	\end{subex}
	
	\begin{subex}
	Now suppose we have a perfectly shuffled big deck, consisting of two \emph{identical} decks of 52 cards (104 cards in total). You cannot tell the difference between, for example, the ace of spades of one deck and the ace of spades of the other. Compute the entropy of the shuffled big deck.
	\end{subex}
\end{exercise}


\begin{exercise}[Mutual information]
Let $X$, $Y$ and $Z$ be random variables such that $I(X;Y) = 0$ and $I(X;Z) = 0$. Does it follow that $I(Y;Z) = 0$? If so, prove it. If not, give a counterexample.
\end{exercise}

\begin{exercise}[Estimating entropy]
(\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Example 2.13:) A source produces a character $x$ from alphabet $\mathcal{A} = \{\mathtt{0, 1, 2, ..., 9, a, b, c, ..., z}\}$. With probability $1/3$, $x$ is a uniformly random numeral $\mathtt{0,1,2,...,9}$, with probability $1/3$, $x$ is a random vowel $\mathtt{a,e,i,o,u}$ and with probability $1/3$, $x$ is one of the 21 consonants. Estimate the entropy of $X$.
\end{exercise}

\begin{exercise}[Geometric distribution]
The geometric($p$) distribution of a random variable $X$ is defined as the number of times one has to flip a Bernoulli$(p)$ coin before it lands on heads:
\[
P_X(k) = (1-p)^{k-1}p \ \ \ \ \ \mbox{for } k = 1, 2, 3, ...
\]
Compute the entropy of the geometric distribution.
\end{exercise}

\begin{exercise}[An optimal code]
Let $X$ be a random variable.
	\begin{subex}
	Show that if there exists an $n \in \mathbb{N}$ such that for all $x \in \mathcal{X}$, $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
	\begin{subex}
	(\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Exercise 5.25:) Show that if for all $x \in \mathcal{X}$, there exists an $n \in \mathbb{N}$ such that $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
\end{exercise}

\begin{exercise}[Stirling's Approximation]
Let $n \in \mathbb{N}$ and $p \in [0,1]$ such that $np \in \mathbb{N}$. Use the approximation $\ln(n!) \approx n \ln(n)$ to prove that
\[
{n \choose np} \approx 2^{n \cdot h(p)},
\]
where $h$ is the binary entropy function.
\end{exercise}

\begin{bonusexercise}[Unique decodability]
Construct a binary symbol code (for a finite alphabet $\mathcal{X}$ of your own choice) that is uniquely decodable, but for which there exists an \emph{infinite} binary string that can be decoded in more than one way.
\end{bonusexercise}












\end{document}