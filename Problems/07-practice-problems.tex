\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{7}
\newcommand\deadline{Friday November 18th, 20:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}

\begin{document}

\practiceproblems

{\sffamily\noindent
This week's exercises deal with noisy-channel coding. \practiceinstructions
}

\begin{exercise}[Symmetric Channels]
Consider the channel with transition matrix
\[
P_{Y|X} = \left[
\begin{array}{c c c}
0.3&0.2&0.5\\
0.5&0.3&0.2\\
0.2&0.5&0.3
\end{array}
\right].
\]
Recall that in a transition matrix, the entry in the $x$th row and the $y$th column denotes the conditional probability $P_{Y|X}(y|x)$ that $y$ is received when $x$ has been sent.

A channel is said to be \textbf{symmetric} if the rows of the channel transition matrix $P_{Y|X}$ are permutations of each other, and the columns are permutations of each other. A channel is said to be \textbf{weakly symmetric} if every row of the transition matrix is a permutation of every other row and all the column sums $\sum_x P_{Y|X}(y|x)$ are equal.

For instance, the channel $P_{Y|X}$ above is symmetric, and the channel
\[
Q_{Y|X} = \left[
\begin{array}{c c c}
\frac{1}{3}&\frac{1}{6}&\frac{1}{2}\\
\frac{1}{3}&\frac{1}{2}&\frac{1}{6}
\end{array}
\right]
\]
is weakly symmetric but not symmetric.
	\begin{subex}
	Find the optimal input distribution and channel capacity of $Q_{Y|X}$.
	\end{subex}
	\begin{subex}
	Give a general strategy how to compute the capacity of weakly symmetric channels. What is the optimal input distribution?
	\end{subex}
\end{exercise}

\begin{exercise}[Multiple Channel Uses]
Prove the lemma below stating that the capacity per transmission is not increased if we use a discrete memoryless channel many times. For inspiration, look again at the proof of the converse of Shannon's noisy-channel coding theorem.

\medskip
\noindent\textbf{Lemma 7.9.2 in [CT]}\  Let $X_1, X_2, \ldots X_n = X^n$ be $n$ random variables with arbitrary joint distribution $P_{X^n}$. Let $Y^n$ be the result of passing $X^n$ through a discrete memoryless channel of capacity $C$. Prove that for all $P_{X^n}$, it holds that $I(X^n; Y^n) \leq nC$.
\medskip

Does your proof also work in case of coding with feedback (i.e.\ $X_{i+1}$ is allowed to depend on $X^i$ and $Y^i$)? If not, point out the steps in your proof where you use that there is no feedback.
\end{exercise}

\begin{exercise}[Z-channel]
The following channel is known as the \emph{Z-channel}:
\begin{center}
\begin{tikzpicture}
\fill[black] (0,0) circle (1mm);
\fill[black] (0,2) circle (1mm);
\fill[black] (2,0) circle (1mm);
\fill[black] (2,2) circle (1mm);
\draw (0,2) -- (2,2);
\node[anchor=south] at (1,2) {1};
\draw (0,0) -- (2,0);
\node[anchor=north] at (1,0) {$1-f$};
\draw (0,0) -- (2,2);
\node[anchor=east] at (1,1) {$f$};

\node[anchor=east] at (0,2) {a};
\node[anchor=east] at (0,0) {b};
\node[anchor=west] at (2,2) {a};
\node[anchor=west] at (2,0) {b};
\end{tikzpicture}
\end{center}
Let $p \in [0,1]$ and write $P_X(a) = 1-p$, $P_X(b) = p$.

\begin{subex}
Express $I(X;Y)$ in terms of $p$ and $f$.
\end{subex}

\begin{subex}
Find the optimal input distribution and the channel capacity for $f = 0.3$.
\end{subex}
\begin{subex}
Do the same for $f = 0.15$ and $f = 0.01$. What do you observe?
\end{subex}
\end{exercise}









\begin{exercise}[Encoder and decoder as part of the channel]
% [CT, 7.16] 
Consider a binary symmetric channel with crossover probability 0.1. A possible coding scheme for this channel with two codewords of length 3 is to encode message $w_1$ as 000 and $w_2$ as 111. Decoding happens by majority vote. With this coding scheme, we can consider the combination of encoder, channel, and decoder as forming a new BSC, with two inputs $w_1$ and $w_2$, and two outputs $w_1$ and $w_2$.

\begin{subex}
Draw the channel and calculate the crossover probability.
\end{subex}

\begin{subex}
What is the capacity of the original channel?
\end{subex}

\begin{subex}
What is the capacity of this new channel in bits per transmission of the original channel? Compare.
\end{subex}

\begin{subex}
Prove the general statement that for any channel, considering the encoder, channel, and decoder together as a new channel from messages to estimated messages will not increase the capacity in bits per transmission of the original channel.
\end{subex}

\end{exercise}

\begin{exercise}[Source and channel]
% [CT, 7.31] 
We wish to encode a Bernoulli($\alpha$) process $V_1, V_2, ...$ for transmission over a binary symmetric channel with crossover probability $\epsilon$.
\begin{center}
\begin{tikzpicture}
\node at (0,0) {$V^n$};
\node at (2,0) {$X^n(V^n)$};
\node at (4,0) {BSC($\epsilon$)};
\node at (6,0) {$Y^n$};
\node at (8,0) {$\hat{V}^n$};
\draw[->, >=latex] (0.5,0) -- (1.25,0);
\draw[->, >=latex] (2.75,0) -- (3.5,0);
\draw[->, >=latex] (4.75,0) -- (5.5,0);
\draw[->, >=latex] (6.5,0) -- (7.5,0);
\end{tikzpicture}
\end{center}
Find conditions on $(\alpha,\epsilon)$ under which the error probability $P[\hat{V}^n \neq V^n]$ can be made to go to zero as $n \to \infty$.
\end{exercise}

\begin{exercise}[Channel with memory]
% [CT, 7.36] 
Consider the discrete memoryless channel $(\mathcal{X}, P_{Y|X}, \mathcal{Y})$ with $\mathcal{X} = \{-1,1\}$, and $Y = ZX$ for a random variable $Z$ with $\mathcal{Z} = \{-1,1\}$.
\begin{subex}
What is the capacity of this channel when $Z$ is uniform?
\end{subex}
\begin{subex}
Now consider the channel with memory. Before transmission begins, $Z$ is randomly chosen and fixed for all time. What is the capacity when $Z$ is uniform?
\end{subex}
\end{exercise}

\begin{exercise}[Practice]
If you have solved all of the above problems before the exercise session is over, take the rest of the time to review previous exercise sets, homeworks or concept quizzes, or to ask any questions that you might have. When done, take on the \href{https://www.moodle.ch/lms/mod/forum/view.php?id=1788}{challenges}.
\end{exercise}













\end{document}