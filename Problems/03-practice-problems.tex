\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{3}
\newcommand\deadline{Friday November 18th, 20:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\begin{document}

\practiceproblems

{\sffamily\noindent
This week's exercises deal with source codes and data compression. \practiceinstructions
}


\begin{exercise}[Prefix-free arithmetic codes]
	\begin{subex}
	What are the names of the binary intervals $[\frac{6}{8},\frac{7}{8})$ and $[\frac{7}{16},\frac{8}{16})$?
	\end{subex}
	\begin{subex}
	What are the binary intervals with the names 0110 and 011?
	\end{subex}
	\begin{subex}
	Prove that if the name of a binary interval $I$ is the prefix of the name of another binary interval $J$, it must be that $J \subset I$.
	\end{subex}
	\begin{subex}
	Use (c) to prove that for any source, the resulting arithmetic code $AC^{pf}$ is indeed prefix-free.
	\end{subex}
\end{exercise}


\begin{exercise}[Comparison of arithmetic codes]
% We have seen procedures to build arithmetic code $AC$ by dividing $[0,1)$ into smaller intervals $I_x$ (for $x \in \mathcal{X}$) according to the probability distribution $P_X$, and picking $AC(x)$ to be shortest possible binary representation of a number in each interval. Furthermore, a prefix-free arithmetic code $AC^{pf}$ for $X$ by dividing $[0,1)$ into smaller intervals $I_x$ (for $x \in \mathcal{X}$) according to the probability distribution $P_X$, and picking $AC^{pf}(x)$ to be the (name of the) largest binary interval that fits into $I_x$.
	\begin{subex}
	Given $X$ with $\mathcal{X} = \{\mathtt{a,b,c,d}\}$ and $P_X(\mathsf{a}) = P_X(\mathsf{b}) = 1/3$, $P_X(\mathsf{c}) = P_X(\mathsf{d}) = 1/6$. Construct the standard arithmetic code $AC$ as well as the prefix-free arithmetic code $AC^{pf}$ for this source. How do the average codeword lengths compare?
	\end{subex}
	\begin{subex}
	Recall the proof that $\ell_{AC}(P_X) \leq H(X) + 1$. Adapt the proof to show that for the prefix-free procedure, the average codeword length $\ell_{AC^{pf}}(P_X)$ is upper bounded by $H(X) + 2$ for any source.
	\end{subex}
\end{exercise}


\begin{exercise}[An optimal code]
Let $X$ be a random variable which takes on values in the finite set $\mathcal{X}$.
	\begin{subex}
	Show that if there exists an $n \in \mathbb{N}_{>0}$ such that for
        all $x \in \mathcal{X}$, $P_X(x) = \frac{1}{2^n}$, then there
        exists a uniquely decodable source code whose expected length equals the entropy.
	\end{subex}
	\begin{subex}
	% (\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Exercise 5.25:) 
        Show that if for all $x \in \mathcal{X}$, there exists an $n \in \mathbb{N}_{>0}$ such that $P_X(x) = \frac{1}{2^n}$, then there exists a uniquely decodable source code whose expected length equals the entropy.
	\end{subex}
\end{exercise}

\begin{bonusexercise}[Unique decodability]
Construct a binary symbol code with a finite number of codewords that is uniquely decodable, but for which there exists an \emph{infinite} binary string that can be decoded in more than one way.
\end{bonusexercise}

\begin{exercise}[Kraft's inequality for Huffman codes]
Prove the following statement. 
If $|\mathcal{X}| >1$, then the word lengths of a binary Huffman code for the source $P_X$ must satisfy Kraft's inequality with equality, i.e. $\sum_i 2^{-\ell_i} = 1$.
\end{exercise}


\begin{exercise}[Optimal codeword lengths]
Although the codeword lengths of an optimal variable-length code are complicated functions of the source probabilities, it can be said that less probable symbols are encoded into longer codewords. Suppose that the message probabilities are given in decreasing order $p_1 > p_2 \geq \cdots \geq p_m$.
\begin{subex}
For a binary code for the above source with the property that all code words have length at least 2, let $C_{xy}$ denote the set of all codewords starting with the two-bitstring $xy \in \{0,1\}^2$, and let $P(C_{xy})$ denote the sum of the message probabilities of codewords in $C_{xy}$. Assume that $C_{00}$ contains the codeword corresponding to message probability $p_1 > 2/5$, and furthermore it holds that $P(C_{01}) \geq P(C_{10}) \geq P(C_{11})$. Find a way to improve the average codeword length of this code.
\end{subex}
\begin{subex}
	Prove that for any binary Huffman code, if the most probable message symbol has probability $p_1 > 2/5$, then that symbol must be assigned a codeword of length 1.
\end{subex}
\begin{subex}
	Prove that for any binary Huffman code, if the most probable message symbol has probability $p_1 < 1/3$, then that symbol must be assigned a codeword of length $\geq 2$.
\end{subex}
\end{exercise}


\end{document}