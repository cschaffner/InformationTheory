\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{3}
\newcommand\deadline{Friday November 18th, 20:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\begin{document}

\practiceproblems

{\sffamily\noindent
This week's exercises deal with source codes and data compression. \practiceinstructions
}

\begin{exercise}[An optimal code]
Let $X$ be a random variable.
	\begin{subex}
	Show that if there exists an $n \in \mathbb{N}$ such that for all $x \in \mathcal{X}$, $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
	\begin{subex}
	% (\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Exercise 5.25:) 
        Show that if for all $x \in \mathcal{X}$, there exists an $n \in \mathbb{N}$ such that $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
\end{exercise}

\begin{bonusexercise}[Unique decodability]
Construct a binary symbol code (for a finite alphabet $\mathcal{X}$ of your own choice) that is uniquely decodable, but for which there exists an \emph{infinite} binary string that can be decoded in more than one way.
\end{bonusexercise}




\begin{exercise}[Prefix-free arithmetic codes]
	\begin{subex}
	What are the names of the binary intervals $[\frac{6}{8},\frac{7}{8})$ and $[\frac{7}{16},\frac{8}{16})$?
	\end{subex}
	\begin{subex}
	What are the binary intervals with the names 0110 and 011?
	\end{subex}
	\begin{subex}
	Prove that if the name of a binary interval $I$ is the prefix of the name of another binary interval $J$, it must be that $J \subset I$.
	\end{subex}
	\begin{subex}
	Use (c) to prove that for any source, the resulting arithmetic code $AC^{pf}$ is indeed prefix-free.
	\end{subex}
\end{exercise}

\begin{exercise}[Comparison of arithmetic codes]
We have seen procedures to build arithmetic code $AC$ by dividing $[0,1)$ into smaller intervals $I_x$ (for $x \in \mathcal{X}$) according to the probability distribution $P_X$, and picking $AC^{pf}(x)$ to be shortest possible binary representation of a number in each interval. Furthermore, a prefix-free arithmetic code $AC^{pf}$ for $X$ by dividing $[0,1)$ into smaller intervals $I_x$ (for $x \in \mathcal{X}$) according to the probability distribution $P_X$, and picking $AC^{pf}(x)$ to be the (name of the) largest binary interval that fits into $I_x$. In this exercise, we consider a simpler procedure that creates slightly shorter codewords, but is not necessarily prefix-free.
	\begin{subex}
	Given $X$ with $\mathcal{X} = \{\mathtt{a,b,c,d}\}$ and $P_X(\mathsf{a}) = P_X(\mathsf{b}) = 1/3$, $P_X(\mathsf{c}) = P_X(\mathsf{d}) = 1/6$. Draw the intervals $I_x$ on $[0,1)$. Then assign codewords to each $x$ by finding a number in each interval with a binary representation that is as short as possible. Note that there are sometimes multiple possibilities!
	\end{subex}
	\begin{subex}
	Also find the prefix-free arithmetic code $AC^{pf}$ for this source. How do the average codeword lengths compare?
	\end{subex}
	\begin{subex}
	Recall the proof that $\ell_{AC}(P_X) \leq H(X) + 1$. Adapt the proof to show that for the prefix-free procedure, the average codeword length $\ell_{AC^{pf}}(P_X)$ is upper bounded by $H(X) + 2$ for any source.
	\end{subex}
\end{exercise}

\begin{exercise}[Sampling from any distribution using random bits]
In this exercise, we come up with a strategy to sample from an arbitrary distribution $P_X$ using fair random bits (for example, the outcome of a sequence of fair coin tosses).
	\begin{subex}
	Let $Z_1$ be a random variable with $\mathcal{Z}_1 = \{a,b,c\}$ and $P_{Z_1}(a) = 1/2$, $P_{Z_1}(b) = P_{Z_1}(c) = 1/4$. Come up with a strategy to sample from $X$ using a number of fair coin tosses. How many coin tosses do you expect to do? How does this compare to the entropy of $Z_1$?
	\end{subex}
	\begin{subex}
	Consider the binary expansion of some $p_i \in [0,1)$. Let the \emph{atoms} of this expansion be the set $At_i := \{2^{-k} \mid \mbox{ the } k^{th} \mbox{ bit of the binary expansion of } p_i \mbox{ is 1.}\}$. Find the atoms for the binary expansion of $p_1 = \frac{1}{3}$ and $p_2 = \frac{2}{3}$.
	\end{subex}
	\begin{subex}
	Show that for any probability distribution with probabilities $(p_1, ..., p_n)$, it is possible to construct a binary tree (the \emph{sampling tree} for this distribution) such that if $2^{-k} \in At_i$ for some $i$, then the tree contains a leaf with label $i$ at depth $k$. \textbf{Hint:} use Kraft's inequality.
	\end{subex}
	\begin{subex}
	Let $Z_2$ be a random variable with $\mathcal{Z}_2 = \{a,b\}$ and $P_{Z_2}(a) = 1/3$, $P_{Z_2}(b) = 2/3$. Construct the sampling tree for $P_{Z_2}$. Find a fair coin and use it to sample from this distribution, following the strategy described by the sampling tree.
	\end{subex}
Let $ET(X)$ denote the expected number of coin tosses when sampling from $X$ using the sampling tree described above. In the rest of this exercise, you will show that this method of sampling from an arbitrary distribution $P_X$ using fair random bits is quite efficient in terms of $ET(X)$.
	\begin{subex}
	Given a sampling tree for an arbitrary distribution $P_X$, define a random variable $Y$ with $\mathcal{Y}$ the set of all leafs of the tree, and $P_Y(y) = 2^{-d(y)}$, where $d(y)$ is the depth of the leaf $y$ in the tree. Prove that $H(Y) = ET(X)$.
	\end{subex}
	\begin{subex}
	Use the result from (e) to prove that $H(X) \leq ET(X)$.
	\end{subex}
	\begin{subex**}
	Prove that $H(Y|X) < 2$ (\textbf{Hint:} see Cover and Thomas, Section 5.12)
	\end{subex**}
	\begin{subex}
	Use the result from $\bigstar$ to prove that $ET(X) < H(X) + 2$.
	\end{subex}
\end{exercise}

\begin{bonusexercise}[Optimal codeword lengths]
(CT, Exercise 5.22) Although the codeword lengths of an optimal variable length code are complicated functions of the source probabilities, it can be said that less probable symbols are encoded into longer codewords. Suppose that the message probabilities are given in decreasing order $p_1 > p_2 \geq \cdots \geq p_m$.
	\begin{subex}
	Prove that for any binary Huffman code, if the most probable message symbol has probability $p_1 > 2/5$, then that symbol must be assigned a codeword of length 1.
	\end{subex}
	\begin{subex}
	Prove that for any binary Huffman code, if the most probable message symbol has probability $p_1 < 1/3$, then that symbol must be assigned a codeword of length $\geq 2$.
	\end{subex}
\end{bonusexercise}










\end{document}