\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{...}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\begin{document}

\practiceproblems

{\sffamily\noindent
%This week's exercises deal with sets, counting and uniform probabilities.
This week's exercises deal with entropy and source codes. You do not have to hand in these exercises, they are for practicing only. Problems marked with a $\bigstar$ are generally a bit harder. If you have questions about any of the exercises, please post them in the \href{https://www.moodle.ch/lms/mod/forum/view.php?id=2219}{discussion forum on Moodle}, and try to help each other. We will also keep an eye on the forum.
}

\begin{exercise}[Mutual information]
Let $X$, $Y$ and $Z$ be random variables such that $I(X;Y) = 0$ and $I(X;Z) = 0$. Does it follow that $I(Y;Z) = 0$? If so, prove it. If not, give a counterexample.
\end{exercise}

\begin{exercise}[Estimating entropy]
(\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Example 2.13:) A source produces a character $x$ from alphabet $\mathcal{A} = \{\mathtt{0, 1, 2, ..., 9, a, b, c, ..., z}\}$. With probability $1/3$, $x$ is a uniformly random numeral $\mathtt{0,1,2,...,9}$, with probability $1/3$, $x$ is a random vowel $\mathtt{a,e,i,o,u}$ and with probability $1/3$, $x$ is one of the 21 consonants. Estimate the entropy of $X$.
\end{exercise}

\begin{exercise}[Geometric distribution]
The geometric($p$) distribution of a random variable $X$ is defined as the number of times one has to flip a Bernoulli$(p)$ coin before it lands on heads:
\[
P_X(k) = (1-p)^{k-1}p \ \ \ \ \ \mbox{for } k = 1, 2, 3, ...
\]
Compute the entropy of the geometric distribution.
\end{exercise}

\begin{exercise}[An optimal code]
Let $X$ be a random variable.
	\begin{subex}
	Show that if there exists an $n \in \mathbb{N}$ such that for all $x \in \mathcal{X}$, $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
	\begin{subex}
	(\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Exercise 5.25:) Show that if for all $x \in \mathcal{X}$, there exists an $n \in \mathbb{N}$ such that $P_X(x) = \frac{1}{2^n}$, then there exists a source code whose expected length equals the entropy.
	\end{subex}
\end{exercise}

\begin{exercise}[Stirling's Approximation]
Let $n \in \mathbb{N}$ and $p \in [0,1]$ such that $np \in \mathbb{N}$. Use the approximation $\ln(n!) \approx n \ln(n)$ to prove that
\[
{n \choose np} \approx 2^{n \cdot h(p)},
\]
where $h$ is the binary entropy function.
\end{exercise}

\begin{bonusexercise}[Unique decodability]
Construct a binary symbol code (for a finite alphabet $\mathcal{X}$ of your own choice) that is uniquely decodable, but for which there exists an \emph{infinite} binary string that can be decoded in more than one way.
\end{bonusexercise}












\end{document}