\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{2}
\newcommand\deadline{...}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments


% Packages
\usepackage[english]{exercises}
\usepackage{wasysym}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor = blue, linkcolor = blue}

\usepackage{tikz}


\begin{document}

\practiceproblems

{\sffamily\noindent
This week's exercises deal with entropy. \practiceinstructions
}

\setcounter{exCount}{-1}
\begin{exercise}[Extra practice problem (not assigned for presentation session): Fun with dice]
	Consider the following random experiment with two fair (regular six-sided) dice. First, the first die is thrown -- let the outcome be $A$. Then, the second die is thrown until the outcome has the same parity (even, odd) as $A$. Let this final outcome of the second die be $B$. The random variables $X$, $Y$ and $Z$ are defined as follows:
	\[
	X = (A + B) \mod 2, \ \ \ \ \ Y = (A \cdot B) \mod 2, \ \ \ \ \ Z = |A - B|.
	\]
	\begin{subex}
		Find the joint distribution $P_{AB}$.
	\end{subex}
	\begin{subex}
		Determine $H(X), H(Y)$ and $H(Z)$.
	\end{subex}
	\begin{subex}
		Compute $H(Z|A=1)$.
	\end{subex}
	\begin{subex}
		Compute $H(AB)$, i.e.\ the joint entropy of $A$ and $B$.
	\end{subex}
	\begin{subex}
		A random variable $M$ describes whether the sum $A + B$ is strictly larger than seven, between five and seven (both included) or strictly smaller than five. How much entropy is present in $M$?
	\end{subex}
\end{exercise}

\begin{exercise}[Properties of entropy]
Let $X$ and $Y$ be random variables.
	\begin{subex}
	Prove that $H(X) = 0$ if and only if $X$ is \emph{constant}, i.e. there is an $x_0 \in \mathcal{X}$ such that $P_X(x_0) = 1$, and $P_X(x') = 0$ for all $x' \neq x_0$.
	\end{subex}
	
	\begin{subex}
	Prove that $H(XY) = H(X) + H(Y)$ if and only if $X$ and $Y$ are independent.
	\end{subex}

	\begin{subex}
	Prove that $H(X) = \log |\mathcal{X}|$ if $X$ is uniformly distributed.
	\end{subex}

	\begin{subex**}
	Prove that $X$ is uniformly distributed if $H(X) = \log |\mathcal{X}|$.
	\end{subex**}
	
\end{exercise}

\begin{exercise}[Entropy of a deck of cards]
	\begin{subex}
	Compute the entropy of a perfectly shuffled deck of 52 cards
        (i.e.\ the set of cards is uniformly distributed over all
        possible orders, not just the entropy of the first card).
	\end{subex}
	
	\begin{subex}
	Now suppose we have a perfectly shuffled big deck, consisting of two \emph{identical} decks of 52 cards (104 cards in total). You cannot tell the difference between, for example, the ace of spades of one deck and the ace of spades of the other. Compute the entropy of the shuffled big deck.
	\end{subex}
\end{exercise}


\begin{exercise}[Mutual information]
	
	\begin{subex}
		Let the binary random variables $X$ and $Y$ be defined by the joint probability distribution $P_{XY}(00) = 1/2$, $P_{XY}(01) = 0$, $P_{XY}(10) = 1/4$, $P_{XY}(11) = 1/4$.
		
		Draw the entropy diagram for $X$ and $Y$, and compute $H(X)$, $H(Y)$, $H(XY)$, $H(X|Y)$, $H(Y|X)$, and $I(X;Y)$. Reading off a value from the diagram is a valid `computation', as long as you clearly state how you did it.
		\begin{center}
			\entropydiagramXY[1]{?}{?}{?}{?}{?}{?}
		\end{center}
	\end{subex}
	
	\begin{subex}
Let $X$, $Y$ and $Z$ be random variables such that $I(X;Y) = 0$ and $I(X;Z) = 0$. Does it follow that $I(Y;Z) = 0$? If so, prove it. If not, give a counterexample.
\end{subex}
\end{exercise}

\begin{exercise}[Estimating entropy]
(\href{http://www.inference.phy.cam.ac.uk/mackay/itila/book.html}{[MacKay]}, Example 2.13:) A source produces a character $x$ from alphabet $\mathcal{A} = \{\mathtt{0, 1, 2, ..., 9, a, b, c, ..., z}\}$. With probability $1/3$, $x$ is a uniformly random numeral $\mathtt{0,1,2,...,9}$, with probability $1/3$, $x$ is a random vowel $\mathtt{a,e,i,o,u}$ and with probability $1/3$, $x$ is one of the 21 consonants. Estimate the entropy of $X$. Can you give a good estimate without using a calculator?
\end{exercise}

\begin{exercise}[Geometric distribution]
The geometric($p$) distribution of a random variable $X$ is defined as the number of times one has to flip a Bernoulli$(p)$ coin before it lands on heads:
\[
P_X(k) = (1-p)^{k-1}p \ \ \ \ \ \mbox{for } k = 1, 2, 3, ...
\]
Compute the entropy of the geometric distribution.
\end{exercise}

\begin{exercise}[Size of binomial coefficient from Stirling's Approximation]
Let $n \in \mathbb{N}$ and $p \in [0,1]$ such that $np \in
\mathbb{N}$. Use
\href{https://en.wikipedia.org/wiki/Stirling's_approximation}{Stirling's
  approximation} of the factorial function $\log(n!) \approx n \log(n)
- n \log(e) $ to prove that
\[
{n \choose np} \approx 2^{n \cdot h(p)},
\]
where $h$ is the binary entropy function.
\end{exercise}













\end{document}